{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>[ADVSTAT] Naive Bayesian Spam Filtering Tutorial</h1> <br>\n",
    "<b>Author:</b> Jan Kristoffer Cheng and Shayane Tan\n",
    "<hr>\n",
    "<h3>Description</h3>\n",
    "<p>In this notebook, we implemented the based on what paper? sino nag sulat ng paper? purpose of the paper? what is the paper about? how was it implemented</p>\n",
    "<h4>References:</h4>\n",
    "<ol>\n",
    "    <li>Androutsopoulos, I., Koutsias, J., Chandrinos, K. V., Paliouras, G., & Spyropoulos, C. D. (2000). An evaluation of naive bayesian anti-spam filtering. arXiv preprint cs/0006013.</li>\n",
    "    <li>Sch√ºtze, H. (2008). 13: Text Classification and Naive Bayes. In Introduction to Information Retrieval (pp. 253-286). Cambridge University Press. Retrieved December 8, 2016, from http://nlp.stanford.edu/IR-book/pdf/13bayes.pdf</li>\n",
    "    <li>Metsis, V., Androutsopoulos, I., & Paliouras, G. (2006, July). Spam filtering with naive bayes-which naive bayes?. In CEAS (pp. 27-28).</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "from model.PartFolder import PartFolder\n",
    "from model.Word import Word\n",
    "from mutual_information.FeatureSelector import FeatureSelector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>To start off, let's import the necessary packages.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Word:\n",
    "    def __init__(self, content):\n",
    "        self.content = content\n",
    "        self.mutualInfo = 0\n",
    "        self.notPresentSpamCount = 0\n",
    "        self.notPresentLegitCount = 0\n",
    "        self.presentSpamCount = 0\n",
    "        self.presentLegitCount = 0\n",
    "        self.spamDocumentCount = 0\n",
    "        self.legitDocumentCount = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The Word class represents each distinct word in the dataset. It contains the information of a particular token  document frequencies for both spam and legitimate categories.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PartFolder:\n",
    "    def __init__(self):\n",
    "        self.spamEmail = []\n",
    "        self.legitEmail = []\n",
    "        self.trainingSpamEmail = []\n",
    "        self.trainingLegitEmail = []\n",
    "        self.relevantWords = [] #relevant words when ith na folder for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The PartFolder class represents the different part folders in the dataset. It categorizes the emails to its corresponding class.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Constant variables\n",
    "FILTERS = ['bare', 'stop', 'lemm', 'lemm_stop']\n",
    "\n",
    "#contains the list of average results per experiment\n",
    "tableResults = []\n",
    "\n",
    "#contains the dataset for the different filter configuration:bare, stop, lemm, lemm_stop\n",
    "filterCollection = {} \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Here, we initialized the necessary variables to contain the words' document frequencies (trainingDistinctWords), training set of spam emails (trainingSpamEmails), training set of legit emails (trainingLegitEmails), and the collection of all preloaded emails or dataset (folderCollection). This is also where the threshold is defined for the spam classification based on the Naive Bayes result. The threshold will be discussed further later.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Preload email dataset:</b>\n",
    "<p>In order to lessen the running time, let us first pre-load the email dataset before training the system and evaluating all results.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loadEmails(path):\n",
    "    print(\"Loading emails...:\",path)\n",
    "\n",
    "    folderCollection = []\n",
    "\n",
    "    #pre-load the emails of each folder per filter configuration\n",
    "    for i in range(1, 11):\n",
    "        partPath = path + str(i)\n",
    "        partFolder = PartFolder()\n",
    "\n",
    "        for filename in os.listdir(partPath):\n",
    "            content = open(partPath + '\\\\' + filename).read()\n",
    "            if filename.startswith('sp'):\n",
    "                partFolder.spamEmail.append(content)\n",
    "            else:\n",
    "                partFolder.legitEmail.append(content)\n",
    "\n",
    "        folderCollection.append(partFolder)\n",
    "\n",
    "    #pre-load the training emails classified as spam and legit\n",
    "    for i in range(10):  # testingIndex\n",
    "        for j in range(10):\n",
    "            if j != i:\n",
    "                folderCollection[i].trainingSpamEmail += folderCollection[j].spamEmail\n",
    "                folderCollection[i].trainingLegitEmail += folderCollection[j].legitEmail\n",
    "\n",
    "    #pre-load and find the relevant words per testing index\n",
    "    for i in range(10):\n",
    "        folderCollection[i].relevantWords = praparingTrainingSet(folderCollection[i])\n",
    "\n",
    "    return folderCollection  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Prepare training dataset:</b>\n",
    "<p>Then, let us prepare our training set by observing the frequencies of the different distinct words.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def praparingTrainingSet(testingFolder):\n",
    "    \n",
    "    trainingDistinctWords = {}\n",
    "\n",
    "    for email in testingFolder.trainingLegitEmail:\n",
    "        email = email.split()\n",
    "        tokenizedEmail = set(email)\n",
    "\n",
    "        # count term frequencies\n",
    "        for token in tokenizedEmail:\n",
    "            if token in trainingDistinctWords:\n",
    "                word = trainingDistinctWords.get(token)\n",
    "                word.presentLegitCount += 1\n",
    "                word.notPresentLegitCount -= 1\n",
    "            else:\n",
    "                word = Word(token)\n",
    "                word.presentLegitCount = 1\n",
    "                word.notPresentLegitCount = len(testingFolder.trainingLegitEmail) - 1\n",
    "                word.presentSpamCount = 0\n",
    "                word.notPresentSpamCount = len(testingFolder.trainingSpamEmail)\n",
    "                trainingDistinctWords[token] = word\n",
    "\n",
    "    for email in testingFolder.trainingSpamEmail:\n",
    "        email = email.split()\n",
    "        tokenizedEmail = set(email)\n",
    "        for token in tokenizedEmail:\n",
    "            if token in trainingDistinctWords:\n",
    "                word = trainingDistinctWords.get(token)\n",
    "                word.presentSpamCount += 1\n",
    "                word.notPresentSpamCount -= 1\n",
    "            else:\n",
    "                word = Word(token)\n",
    "                word.presentSpamCount = 1\n",
    "                word.notPresentSpamCount = len(testingFolder.trainingSpamEmail) - 1\n",
    "                word.presentLegitCount = 0\n",
    "                word.notPresentLegitCount = len(testingFolder.trainingLegitEmail)\n",
    "                trainingDistinctWords[token] = word\n",
    "\n",
    "    fs = FeatureSelector(trainingDistinctWords)\n",
    "    return fs.getRelevantWords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def selectNFeatures(nFeatures, testingFolder):\n",
    "    relevantWords = {x[0]: x[1] for x in testingFolder.relevantWords[:nFeatures]}\n",
    "\n",
    "    for key in relevantWords:\n",
    "        word = relevantWords[key]\n",
    "\n",
    "        for email in testingFolder.trainingSpamEmail:\n",
    "            if word.content in email:\n",
    "                word.spamDocumentCount += 1  # count document frequencies\n",
    "\n",
    "        for email in testingFolder.trainingLegitEmail:\n",
    "            if word.content in email:\n",
    "                word.legitDocumentCount += 1  # count document frequencies\n",
    "\n",
    "    return relevantWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeNaiveBayes(testingFolder, emailContent, relevantWords):\n",
    "    # Naive Bayes: Multinomial NB, TF attributes\n",
    "    emailContent = emailContent.split()\n",
    "    dict_testingData = {}  # dictionary of distinct words in testing data\n",
    "\n",
    "    total_trainingEmails = len(testingFolder.trainingSpamEmail) + len(testingFolder.trainingLegitEmail)\n",
    "\n",
    "    probIsSpam = len(testingFolder.trainingSpamEmail) / total_trainingEmails\n",
    "    probIsLegit = len(testingFolder.trainingLegitEmail) / total_trainingEmails\n",
    "\n",
    "    probWord_isPresentSpam = 1.0\n",
    "    probWord_isPresentLegit = 1.0\n",
    "\n",
    "    # determine whther term appeared in document\n",
    "    for key in relevantWords:\n",
    "        if key in emailContent:\n",
    "            dict_testingData[key] = 1\n",
    "        else:\n",
    "            dict_testingData[key] = 0\n",
    "\n",
    "    for key in relevantWords:\n",
    "        word = relevantWords[key]\n",
    "        power = dict_testingData[key]\n",
    "\n",
    "        prob_t_s = (1 + word.spamDocumentCount) / (2 + len(testingFolder.trainingSpamEmail))\n",
    "        prob_t_l = (1 + word.legitDocumentCount) / (2 + len(testingFolder.trainingLegitEmail))\n",
    "\n",
    "        probWord_isPresentSpam *= (math.pow(prob_t_s, power) * math.pow(1 - prob_t_s, 1 - power))\n",
    "        probWord_isPresentLegit *= (math.pow(prob_t_l, power) * math.pow(1 - prob_t_l, 1 - power))\n",
    "\n",
    "    return (probIsSpam * probWord_isPresentSpam) / ( probIsSpam * probWord_isPresentSpam + probIsLegit * probWord_isPresentLegit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Result:\n",
    "    def __init__(self):\n",
    "        self.filter_config = ''\n",
    "        self.threshold = 0\n",
    "        self.nFeatures = 0\n",
    "        self.avg_recall = 0.0\n",
    "        self.avg_precision = 0.0\n",
    "        self.avg_w_acc = 0.0\n",
    "        self.avg_bw_acc = 0.0\n",
    "        self.avg_tcr = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function for constructing the average results table per filter, nAttributes, and threshold configuration\n",
    "def runTestTable(filter, threshold, nFeatures):\n",
    "    \n",
    "    print(\"Testing filter:\", filter,\" threshold:\", threshold, \" No. of Attributes:\", nFeatures)\n",
    "    folderCollection = filterCollection[filter]\n",
    "    threshold_lambda = threshold\n",
    "    threshold = threshold_lambda /(1+threshold_lambda)\n",
    "\n",
    "\n",
    "    sPrecision = 0\n",
    "    sRecall = 0\n",
    "    wAcc_b = 0\n",
    "    wErr_b = 0\n",
    "    wAcc = 0\n",
    "    wErr = 0\n",
    "    tcr = 0\n",
    "\n",
    "    for testingIndex in range(10):\n",
    "        relevantWords = selectNFeatures(nFeatures, folderCollection[testingIndex])\n",
    "\n",
    "        s_s = 0 #spam email categorized as spam\n",
    "        s_l = 0 #spam email categorized as legit\n",
    "        l_s = 0 #legit email categorized as spam\n",
    "        l_l = 0 #legit email categorized as legit\n",
    "\n",
    "        spamSize = len(folderCollection[testingIndex].spamEmail)\n",
    "        legitSize = len(folderCollection[testingIndex].legitEmail)\n",
    "\n",
    "        for email in folderCollection[testingIndex].spamEmail:\n",
    "            result = computeNaiveBayes(folderCollection[testingIndex], email, relevantWords)\n",
    "            if result > threshold: #isSpam\n",
    "                s_s += 1\n",
    "            else: #isLegit\n",
    "                s_l += 1\n",
    "\n",
    "\n",
    "        for email in folderCollection[testingIndex].legitEmail:\n",
    "            result = computeNaiveBayes(folderCollection[testingIndex], email, relevantWords)\n",
    "            if result > threshold: #isSpam\n",
    "                l_s += 1\n",
    "            else:\n",
    "                l_l += 1\n",
    "\n",
    "        sPrecision += (s_s / (s_s + l_s))\n",
    "        sRecall += (s_s / (s_s +s_l))\n",
    "        wAcc += (threshold_lambda * l_l + s_s)/ (threshold_lambda * legitSize + spamSize)\n",
    "        wErr += (threshold_lambda * l_s + s_l)/ (threshold_lambda * legitSize + spamSize)\n",
    "        wAcc_b += (threshold_lambda * legitSize)/(threshold_lambda * legitSize + spamSize)\n",
    "        wErr_b += spamSize / (threshold_lambda * legitSize + spamSize)\n",
    "        tcr += spamSize / (threshold_lambda*l_s + s_l)\n",
    "\n",
    "\n",
    "    table_row = Result()\n",
    "    table_row.filter_config = filter\n",
    "    table_row.threshold = threshold_lambda\n",
    "    table_row.nFeatures = nFeatures\n",
    "    table_row.avg_recall =  (sRecall/10)*100\n",
    "    table_row.avg_precision = (sPrecision/10)*100\n",
    "    table_row.avg_w_acc = (wAcc/10)*100\n",
    "    table_row.avg_bw_acc = (wAcc_b/10)*100\n",
    "    table_row.avg_tcr = tcr/10\n",
    "    \n",
    "    print(\"S_Precision:\", (sPrecision/10)*100)\n",
    "    print(\"S_Recall:\", (sRecall/10)*100)\n",
    "    print(\"w_acc:\", (wAcc/10)*100)\n",
    "    print(\"w_accB:\", (wAcc_b/10)*100)\n",
    "    print(\"TCR:\", tcr/10)\n",
    "    \n",
    "    tableResults.append(table_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def table_row_generator(pd, filter, threshold, nFeatures, avg_recall, avg_precision, avg_accuracy, avg_accuracy_base,\n",
    "                        avg_tcr):\n",
    "    raw_data = {\n",
    "        'Filter Configuration': [filter],\n",
    "        'Lambda': [threshold],\n",
    "        'No. of attrib.': [nFeatures],\n",
    "        'Spam Recall': [avg_recall],\n",
    "        'Spam Precision': [avg_precision],\n",
    "        'Weighted Accuracy': [avg_accuracy],\n",
    "        'Baseline W. Acc': [avg_accuracy_base],\n",
    "        'TCR': [avg_tcr]\n",
    "    }\n",
    "    return pd.DataFrame(raw_data, columns=['Filter Configuration', 'Lambda', 'No. of attrib.', 'Spam Recall',\n",
    "                                           'Spam Precision', 'Weighted Accuracy', 'Baseline W. Acc', 'TCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading emails...: spam emails\\bare\\part\n",
      "Loading emails...: spam emails\\stop\\part\n",
      "Loading emails...: spam emails\\lemm\\part\n",
      "Loading emails...: spam emails\\lemm_stop\\part\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(FILTERS)):\n",
    "    filterCollection[FILTERS[i]] = loadEmails('spam emails\\\\'+FILTERS[i]+'\\\\part')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table results size: 0\n",
      "Testing filter: bare  threshold: 1  No. of Attributes: 50\n",
      "S_Precision: 98.85416666666667\n",
      "S_Recall: 64.27295918367348\n",
      "w_acc: 93.91932086608928\n",
      "w_accB: 83.37378155712214\n",
      "TCR: 3.034235098235098\n",
      "Testing filter: stop  threshold: 1  No. of Attributes: 50\n",
      "S_Precision: 99.12186379928315\n",
      "S_Recall: 64.8937074829932\n",
      "w_acc: 94.05749135563136\n",
      "w_accB: 83.37378155712214\n",
      "TCR: 3.054600443416233\n",
      "table results size: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filter Configuration</th>\n",
       "      <th>Lambda</th>\n",
       "      <th>No. of attrib.</th>\n",
       "      <th>Spam Recall</th>\n",
       "      <th>Spam Precision</th>\n",
       "      <th>Weighted Accuracy</th>\n",
       "      <th>Baseline W. Acc</th>\n",
       "      <th>TCR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bare</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>64.272959</td>\n",
       "      <td>98.854167</td>\n",
       "      <td>93.919321</td>\n",
       "      <td>83.373782</td>\n",
       "      <td>3.034235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stop</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>64.893707</td>\n",
       "      <td>99.121864</td>\n",
       "      <td>94.057491</td>\n",
       "      <td>83.373782</td>\n",
       "      <td>3.054600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Filter Configuration  Lambda  No. of attrib.  Spam Recall  Spam Precision  \\\n",
       "0                 bare       1              50    64.272959       98.854167   \n",
       "0                 stop       1              50    64.893707       99.121864   \n",
       "\n",
       "   Weighted Accuracy  Baseline W. Acc       TCR  \n",
       "0          93.919321        83.373782  3.034235  \n",
       "0          94.057491        83.373782  3.054600  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tableResults = []\n",
    "print(\"table results size:\", len(tableResults))\n",
    "runTestTable(FILTERS[0], 1, 50)\n",
    "runTestTable(FILTERS[1], 1, 50)\n",
    "print(\"table results size:\", len(tableResults))\n",
    "#1:53\n",
    "\n",
    "\n",
    "table_row = []\n",
    "for i in range(len(tableResults)):\n",
    "    row = tableResults[i]\n",
    "    df = table_row_generator(pd, row.filter_config, row.threshold, row.nFeatures, row.avg_recall, row.avg_precision, \n",
    "                            row.avg_w_acc, row.avg_bw_acc, row.avg_tcr)\n",
    "\n",
    "table = pd.concat(table_row)\n",
    "table"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
